<!doctype html>
<html lang="en" dir="ltr" class="mdx-wrapper mdx-page plugin-pages plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Water Segmentation | Sage Website</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://nconrad.github.io/waggle-docs/science/water-segmentation"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Water Segmentation | Sage Website"><meta data-rh="true" name="description" content="Water and Our Environment"><meta data-rh="true" property="og:description" content="Water and Our Environment"><link data-rh="true" rel="icon" href="/waggle-docs/img/sage-favicon.png"><link data-rh="true" rel="canonical" href="https://nconrad.github.io/waggle-docs/science/water-segmentation"><link data-rh="true" rel="alternate" href="https://nconrad.github.io/waggle-docs/science/water-segmentation" hreflang="en"><link data-rh="true" rel="alternate" href="https://nconrad.github.io/waggle-docs/science/water-segmentation" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://MPDJQF6T11-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/waggle-docs/blog/rss.xml" title="Sage Website RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/waggle-docs/blog/atom.xml" title="Sage Website Atom Feed">



<link rel="search" type="application/opensearchdescription+xml" title="Sage Website" href="/waggle-docs/opensearch.xml"><link rel="stylesheet" href="/waggle-docs/assets/css/styles.680360fe.css">
<link rel="preload" href="/waggle-docs/assets/js/runtime~main.ebb2da0c.js" as="script">
<link rel="preload" href="/waggle-docs/assets/js/main.09bb6aac.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/waggle-docs/"><div class="navbar__logo"><img src="/waggle-docs/img/logo.png" alt="Site Logo" class="themedImage_ToTc themedImage--light_HNdA custom-navbar-logo-class"><img src="/waggle-docs/img/logo_dark.svg" alt="Site Logo" class="themedImage_ToTc themedImage--dark_i4oU custom-navbar-logo-class"></div><b class="navbar__title text--truncate">Sage</b></a><a class="navbar__item navbar__link" href="/waggle-docs/about">About</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/waggle-docs/science">Science</a><a class="navbar__item navbar__link" href="/waggle-docs/blog">News</a><a class="navbar__item navbar__link" href="/waggle-docs/publications">Publications</a><a class="navbar__item navbar__link" href="/waggle-docs/team">Team</a><a class="navbar__item navbar__link" href="/waggle-docs/docs/about/overview">Docs</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link header-github-link" aria-label="GitHub repository"></a><ul class="dropdown__menu"><li><a href="https://github.com/sagecontinuum" target="_blank" rel="noopener noreferrer" class="dropdown__link">Sage GitHub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/waggle-sensor" target="_blank" rel="noopener noreferrer" class="dropdown__link">Waggle Sensor GitHub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="SignInBtn__Root-sc-1lj79ef-0 fqsrYB"><style data-emotion="css 6ii3fu">.css-6ii3fu{font-family:"Roboto","Helvetica","Arial",sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:5px 15px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border:1px solid rgba(25, 118, 210, 0.5);color:#1976d2;}.css-6ii3fu:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(25, 118, 210, 0.04);border:1px solid #1976d2;}@media (hover: none){.css-6ii3fu:hover{background-color:transparent;}}.css-6ii3fu.Mui-disabled{color:rgba(0, 0, 0, 0.26);border:1px solid rgba(0, 0, 0, 0.12);}</style><style data-emotion="css 79xub">.css-79xub{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;box-sizing:border-box;-webkit-tap-highlight-color:transparent;background-color:transparent;outline:0;border:0;margin:0;border-radius:0;padding:0;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;-moz-appearance:none;-webkit-appearance:none;-webkit-text-decoration:none;text-decoration:none;color:inherit;font-family:"Roboto","Helvetica","Arial",sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:5px 15px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border:1px solid rgba(25, 118, 210, 0.5);color:#1976d2;}.css-79xub::-moz-focus-inner{border-style:none;}.css-79xub.Mui-disabled{pointer-events:none;cursor:default;}@media print{.css-79xub{-webkit-print-color-adjust:exact;color-adjust:exact;}}.css-79xub:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(25, 118, 210, 0.04);border:1px solid #1976d2;}@media (hover: none){.css-79xub:hover{background-color:transparent;}}.css-79xub.Mui-disabled{color:rgba(0, 0, 0, 0.26);border:1px solid rgba(0, 0, 0, 0.12);}</style><a class="MuiButtonBase-root MuiButton-root MuiButton-outlined MuiButton-outlinedPrimary MuiButton-sizeMedium MuiButton-outlinedSizeMedium MuiButton-root MuiButton-outlined MuiButton-outlinedPrimary MuiButton-sizeMedium MuiButton-outlinedSizeMedium css-79xub" tabindex="0" href="https://portal.sagecontinuum.org">Portal<style data-emotion="css 1n4a93h">.css-1n4a93h{display:inherit;margin-right:-4px;margin-left:8px;}.css-1n4a93h>*:nth-of-type(1){font-size:20px;}</style><span class="MuiButton-endIcon MuiButton-iconSizeMedium css-1n4a93h"><style data-emotion="css vubbuv">.css-vubbuv{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:1em;height:1em;display:inline-block;fill:currentColor;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;font-size:1.5rem;}</style><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-vubbuv" focusable="false" aria-hidden="true" viewBox="0 0 24 24" data-testid="LoginRoundedIcon"><path d="M10.3 7.7c-.39.39-.39 1.01 0 1.4l1.9 1.9H3c-.55 0-1 .45-1 1s.45 1 1 1h9.2l-1.9 1.9c-.39.39-.39 1.01 0 1.4.39.39 1.01.39 1.4 0l3.59-3.59c.39-.39.39-1.02 0-1.41L11.7 7.7a.9839.9839 0 0 0-1.4 0zM20 19h-7c-.55 0-1 .45-1 1s.45 1 1 1h7c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2h-7c-.55 0-1 .45-1 1s.45 1 1 1h7v14z"></path></svg></span></a></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><main class="container container--fluid margin-vert--lg"><div class="row mdxPageWrapper_j9I6"><div class="col col--8"><article><h1>Water Segmentation</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="water-and-our-environment">Water and Our Environment<a href="#water-and-our-environment" class="hash-link" aria-label="Direct link to Water and Our Environment" title="Direct link to Water and Our Environment">​</a></h2><p>It goes without saying that water is an important part of our environment. Water supports entire ecosystems of animals and is essential to human life. It is one of the most powerful driving forces of nature. But — as anyone who has fought a losing battling against flooding will tell you — it can also be a destructive force. Hurricanes and urban flooding can pose serious hazards to the people who live in harm’s way.1 Fortunately, however, we can utilize the power of edge computing to gather insights into the behavior of water to minimize its destruction. The Sage infrastructure will be able to equip hydrologists with the ability to study water in high-risk areas in real time.</p><p>My name is Luke Jacobs and I am a rising sophmore at the University of Illinois at Urbana-Champaign. My work within Sage has been researching water segmentation with computer vision and machine learning. The end goal of my work is to develop a general-purpose water segmentation tool that will allow Sage nodes to “see” water in their surroundings. Once this algorithm can be deployed onto Sage nodes, domain scientists like hydrologists will be able to gather hourly data on the level of water present in any environment containing a node. This software will be able to provide them with time-series and location-based data on issues like urban flooding to allow them to better understand how water affects our environment.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="a-visual-model-of-water">A Visual Model of Water<a href="#a-visual-model-of-water" class="hash-link" aria-label="Direct link to A Visual Model of Water" title="Direct link to A Visual Model of Water">​</a></h2><p><img loading="lazy" alt="sage node water seg" src="/waggle-docs/assets/images/water-seg-1-e9558ba40250ca5ab9caa22f2d0d9f74.jpg" width="902" height="439" class="img_ev3q"></p><blockquote><p>An image taken from a Sage node in Chicago</p></blockquote><p>An image taken from a Sage node in Chicago
When you look at the above picture, it takes almost no time at all for you to notice the regions of standing water on this field. That’s the way it should be, since “half of the human brain is devoted directly or indirectly to vision,” explains Professor Mriganka Sur of MIT’s Department of Brain and Cognitive Sciences.2 A task like identifying water is absolutely trivial to us, but to a computer which has no native understanding of 3-dimensional space, teaching it to “see” is a very challenging task. In fact, the above image is one of the hardest for computers to segment (or outline), since it requires an understanding of context, lighting, texture, and color cues all working together.</p><p>As humans, we draw on numerous visual cues to identify water. In my research, I programmed classifiers for three cues which, when combined, can help computers distinguish between regions of water and other scenery. These cues are color, texture, and motion.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="color">Color<a href="#color" class="hash-link" aria-label="Direct link to Color" title="Direct link to Color">​</a></h2><p>For color identification I researched the use of Gaussian Mixture Models.3 This classifier was able to, for each pixel in a given image, output a probability value that that pixel was a water pixel. This initial classifier was a naive but efficient method for filtering out areas in an image that the computer was certain could not contain water. Take the following images, for example:</p><p><img loading="lazy" alt="segmentation color" src="/waggle-docs/assets/images/water-seg-2-9e93af1a13cbbb961e067336a610f5d5.png" width="775" height="451" class="img_ev3q"></p><blockquote><p>Two example images and their segmentation outputs (original images taken from the Video Water Database by Mettes, et al. 2015)</p></blockquote><p>The bottom row are input images to the color classifier. The top row images are the segmentation results of the classifier where yellow pixels correspond to a high probability of water and the dark blue pixels correspond to a low probability of water. As you can see in the first row, the classifier was quite easily able to distinguish the buoy from the ocean. However, example images like these are trivial to segment. Segmenting water by color alone would be highly inaccurate, since water can reflect any color. The main role of the color classifier, then, is to flag areas of an image that cannot possibly be water, like the red buoy.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="texture">Texture<a href="#texture" class="hash-link" aria-label="Direct link to Texture" title="Direct link to Texture">​</a></h2><p>Because simple color information is not a strong enough cue by itself to classify water spatially, I looked into a texture-based classifier using Local Binary Patterns (LBP’s). Local Binary Patterns are a way of representing an image that highlights its low-level grayscale gradients. In the case of water, this is quite helpful, since ripples and waves in water have distinct texture to them.</p><p><img loading="lazy" alt="texture" src="/waggle-docs/assets/images/water-seg-3-5827118e308ee64ae683442db1ae3d1e.png" width="773" height="345" class="img_ev3q"></p><p>Intuitively, we can look at these images and say that the one on the left is probably an ocean wave and the one on the right might be a puddle. This is because water lies flat on the ground and exhibits ripples. One of the most helpful questions that I have had to ask myself throughout this research has been: How do I know this is water? If I can deduce why my brain thinks what I am looking at is water, then I can program that same line of thinking in a device.</p><p>Each LBP image segmented by the texture classifier was divided into a number of 10 pixel by 10 pixel blocks, which then were separately classified by a Random Forest classifier. This yielded some good results, albeit with some noise:</p><p><img loading="lazy" alt="texture" src="/waggle-docs/assets/images/water-seg-4-81629a35e43eff835b91b32ebe608663.png" width="2048" height="387" class="img_ev3q"></p><blockquote><p>Original images and segmentation masks (original images taken from the Video Water Database by Mettes, et al. 2015)</p></blockquote><p>The black-and-white boxy-looking images represent the predictions of the texture classifier, with the original image to the left. As you can see in the two right-most columns, the classifier can discern a significant difference between a image of a reflection of trees on a pond and an actual image of trees! It is important to have the classifier learn this level of visual subtlety, because it needs to be able to flag reflections as water.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="motion">Motion<a href="#motion" class="hash-link" aria-label="Direct link to Motion" title="Direct link to Motion">​</a></h2><p>Color and texture are helpful visual cues of water, but what really sets water apart from other scenery is the way it moves. The motion classifier, proposed by the paper “Water detection through spatio-temporal invariant descriptors,” is the most sophisticated method for water segmentation that I researched. It involves multiple preprocessing steps, which, although relatively computationally expensive, yield impressive segmentation results:</p><p><img loading="lazy" alt="texture" src="/waggle-docs/assets/images/water-seg-5-1f88204e95b7592c052f8d59154fa0ea.png" width="2048" height="507" class="img_ev3q"></p><blockquote><p>Original images and segmentation masks (original images taken from the Video Water Database by Mettes, et al. 2015)
The motion classifier can draw clear divisions between rivers and their boundaries, since it relies on the pixel-level motion of water to make segmentation predictions. This is helpful in supplementing the texture classifier, which struggles with noise, especially around the boundaries of water bodies.</p></blockquote><p>The motion classifier works by first dividing a set of ~60 grayscale frames into many video “patches.” Each video patch represents a 3 pixel by 3 pixel space in the video. After the video is broken up into patches, the spatial mean of the grayscale intensity of each patch is computed over the dimension of time. This yields a large number of signals which can then be fed into a discrete fourier transform, the output of which is finally fed into a Random Forest classifier.</p><p><img loading="lazy" alt="texture" src="/waggle-docs/assets/images/water-seg-6-f1f4a1fc14765d9542bd7bf52fb88de1.png" width="1099" height="740" class="img_ev3q"></p><p>The purpose of these preprocessing steps is to allow the Random Forest to perceive the specific motion of small regions of a video. It is able to automatically learn the unique visual frequency bands that characterize the ripples and waves in water, allowing it to identify water with only just a second or two of video footage.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="one-classifier-to-rule-them-all">One Classifier to Rule Them All<a href="#one-classifier-to-rule-them-all" class="hash-link" aria-label="Direct link to One Classifier to Rule Them All" title="Direct link to One Classifier to Rule Them All">​</a></h2><p>The end goal of water segmentation research is to build a general purpose segmentation algorithm. This algorithm should ideally be able to recognize water in any situation from any perspective with just a second or two of input frames. Although the current algorithm works well on labeled datasets, it is not finished. Future work is necessary to boost its accuracy and implement it on Sage nodes.</p><p>Water can be a destructive force, but edge computing will make it possible to monitor its behavior and minimize its damage through intelligent sensing.</p><p>Thanks for reading!</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><ol><li>On average, 1.9 significant flood or torrential rain events occur each year, and $362 million of losses to insured property are incurred each year from such events. (FEMA, <a href="https://www.ncbi.nlm.nih.gov/books/NBK541178/" target="_blank" rel="noopener noreferrer">https://www.ncbi.nlm.nih.gov/books/NBK541178/</a>)</li><li>Quote found at <a href="http://news.mit.edu/1996/visualprocessing" target="_blank" rel="noopener noreferrer">http://news.mit.edu/1996/visualprocessing</a></li><li>Rankin, A. &amp; Matthies, L.. (2010). Daytime water detection based on color variation. 215 – 221. 10.1109/IROS.2010.5650402.</li><li>Mettes, Pascal &amp; Tan, Robby &amp; Veltkamp, Remco. (2015). Water Detection through Spatio-Temporal Invariant Descriptors. Computer Vision and Image Understanding. 154. 10.1016/j.cviu.2016.04.003.</li></ol></article></div><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#water-and-our-environment" class="table-of-contents__link toc-highlight">Water and Our Environment</a></li><li><a href="#a-visual-model-of-water" class="table-of-contents__link toc-highlight">A Visual Model of Water</a></li><li><a href="#color" class="table-of-contents__link toc-highlight">Color</a></li><li><a href="#texture" class="table-of-contents__link toc-highlight">Texture</a></li><li><a href="#motion" class="table-of-contents__link toc-highlight">Motion</a></li><li><a href="#one-classifier-to-rule-them-all" class="table-of-contents__link toc-highlight">One Classifier to Rule Them All</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></main></div><footer class="footer"><div class="container container-fluid"></div></footer></div>
<script src="/waggle-docs/assets/js/runtime~main.ebb2da0c.js"></script>
<script src="/waggle-docs/assets/js/main.09bb6aac.js"></script>
</body>
</html>