"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[891],{3905:(e,t,a)=>{a.d(t,{Zo:()=>h,kt:()=>m});var n=a(7294);function o(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function r(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){o(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,o=function(e,t){if(null==e)return{};var a,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(o[a]=e[a]);return o}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(o[a]=e[a])}return o}var c=n.createContext({}),l=function(e){var t=n.useContext(c),a=t;return e&&(a="function"==typeof e?e(t):r(r({},t),e)),a},h=function(e){var t=l(e.components);return n.createElement(c.Provider,{value:t},e.children)},d="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,o=e.mdxType,i=e.originalType,c=e.parentName,h=s(e,["components","mdxType","originalType","parentName"]),d=l(a),p=o,m=d["".concat(c,".").concat(p)]||d[p]||u[p]||i;return a?n.createElement(m,r(r({ref:t},h),{},{components:a})):n.createElement(m,r({ref:t},h))}));function m(e,t){var a=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var i=a.length,r=new Array(i);r[0]=p;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s[d]="string"==typeof e?e:o,r[1]=s;for(var l=2;l<i;l++)r[l]=a[l];return n.createElement.apply(null,r)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},4878:(e,t,a)=>{a.r(t),a.d(t,{contentTitle:()=>r,default:()=>d,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var n=a(7462),o=(a(7294),a(3905));const i={},r="Vehicle Tracking",s={type:"mdx",permalink:"/sage-website/science/vehicle-tracking",source:"@site/src/pages/science/vehicle-tracking.md",title:"Vehicle Tracking",description:"Hi there!",frontMatter:{}},c=[{value:"Tracking-by-Detection",id:"tracking-by-detection",level:3},{value:"Conclusion",id:"conclusion",level:2},{value:"References",id:"references",level:2}],l={toc:c},h="wrapper";function d(e){let{components:t,...i}=e;return(0,o.kt)(h,(0,n.Z)({},l,i,{components:t,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"vehicle-tracking"},"Vehicle Tracking"),(0,o.kt)("p",null,"Hi there!\nMy name is Hazel Han, and I\u2019m a graduate student at Purdue University pursuing MS in Computer and Information Technology.\nThis summer, I worked as a research intern at Argonne National Laboratory on a \u201cVehicle Tracking\u201d project, pursuing the ultimate goal of running tracker at the edge of the smart sensor network, and I\u2019d like to share what I\u2019ve done throughout the summer."),(0,o.kt)("p",null,"#Smart City? Urban Surveillance?\nVehicle is one of the most useful and indispensable machines in our lives. Every day people drive to work, goods are transported by vehicles, and people even play sports with vehicles. Seriously, I can\u2019t even imagine life without vehicles. However, as they are so close to human lives, there are also multifarious problems that vehicles are involved with, such as traffic congestion, vehicle theft and trafficking, and hit-and-run. One way to deal with those vehicle-related problems is to track each vehicle. An intelligent traffic surveillance system that could track each vehicle and figure out the trajectory of each vehicle will play a significant role in various tasks such as traffic flow analysis or catching hit-and-run criminals. Therefore, based on such needs for an intelligent traffic surveillance system, I\u2019ve conducted research on developing a near real-time vehicle tracking model that could be used towards the intelligent traffic surveillance system."),(0,o.kt)("p",null,"With the growth of the Internet of Things (IoT) related technology, a lot of governments, national laboratories, and companies are to construct a smart city infrastructure these days. (Su, Li & Fu, 2011). Argonne National Laboratory is one of those institutions. Beckman et al. at Argonne National Laboratory have been developing an open platform named Sage, a smart software-defined sensor network, that could be used for various civil applications and AI-related researches. So during this summer, I worked on implementing a deep learning-based vehicle tracking model that could be deployed at the edge of Sage, and I\u2019d like to share a high-level description about my model in this post."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"tracking traffic",src:a(7567).Z,width:"1280",height:"720"})),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Figure 1. Tracking on random traffic video from YouTube")),(0,o.kt)("h1",{id:"multiple-object-tracking"},"Multiple Object Tracking"),(0,o.kt)("p",null,"Before we go into details of the model I\u2019ve implemented, I\u2019d like to first introduce how the general computer vision-based tracking algorithms work. Although there are various approaches for developing computer vision-based tracking systems, the gist of tracking algorithms is pretty much the same for every tracking algorithms.\nMost of the tracking systems that have been proposed since 1990s are feature-based tracking systems, which use feature vectors extracted from the image frames to associate certain feature vector from certain image frame with another similar feature vector extracted from another frame.\nThe general tracking scenario is as follows:"),(0,o.kt)("p",null,"Feature extraction from each frame\nMeasure the similarity between feature vectors\nAssociate those similar vectors with unique tracking ID (similar feature vector = the same object)\nHaving this basic scenario, various methods and techniques have been explored by researchers to come up with better performing tracking algorithms. However, while the object detection task has been continuously addressed by the computer vision community with well-organized benchmarks for the task since the early 2010s (ImageNet), there has been rather limited work on the task of object tracking although it can be used for various applications. To address such issue, Leal-Taixe \u0301 et al. have launched Multiple Object Tracking Challenge (MOTChallenge) in 2015, in order to provide a novel multiple object tracking benchmark, which consists of a large-scale well-annotated dataset and a unified evaluation framework for effective quantification of multi-object tracking."),(0,o.kt)("p",null,"Since the first MOTChallenge, a significant number of researchers have participated and contributed to the challenge, proposing state-of-the-art tracking algorithms, and there is an obvious trend in submitted algorithms. Most of the MOT algorithms take tracking-by-detection approach.\nTo be more specific, a set of detections (i.e. bounding boxes identifying the targets in the image) are extracted from the video frames and are used to guide the tracking process, usually by associating them together in order to assign the same ID to bounding boxes that contain the same target.\nLet\u2019s take an example of a pedestrian tracking shown in the below figure:"),(0,o.kt)("h3",{id:"tracking-by-detection"},"Tracking-by-Detection"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"workflow of MOT algorithms",src:a(7734).Z,width:"1226",height:"652"})),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Figure 2. Usual workflow of MOT Algorithms (Ciaparrone et al., 2020)")),(0,o.kt)("p",null,"Given the raw frames of a video (1), an object detector is run to obtain the bounding boxes of the objects (2). Then, for every detected object, different features are computed, usually visual and motion ones (3). After that, an affinity computation step calculates the probability of two objects belonging to the same target (4), and finally an association step assigns a numerical ID to each object (5). (Ciapparrone et al., 2020)"),(0,o.kt)("p",null,"So this is how most of the current state-of-the-art tracking algorithms work."),(0,o.kt)("h1",{id:"deep-sort-based-vehicle-tracker"},"Deep SORT based Vehicle Tracker"),(0,o.kt)("p",null,"Now I\u2019d like to introduce a specific tracker, Deep SORT, that I used as a baseline. Deep SORT is one of the fastest trackers that takes a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. (Wojke et al., 2017). Following the general tracking scenario that I just described in the previous section, Deep SORT utilizes Kalman Filtering and Hungarian Algorithm to expedite the speed of tracker. Based on Deep SORT, which intends to track pedestrians from the given videos, I have customized and implemented the tracking model which tracks vehicles.\nThe detailed model flow is depicted in Figure 3. The model has four stages \u2013 detection stage, feature extraction/motion prediction stage, affinity stage, and association stage \u2013 and here is the detailed explanation about each stage:"),(0,o.kt)("p",null,"Detection stage: detect objects from each input frame with state-of-the-art object detectors, and generate bounding boxes around the detected objects; YOLOv3\nFeature extraction/motion prediction stage: a feature extractor extracts feature vector of target objects (NVIDIA AI City Challenge Re-Identification dataset used), and a motion predictor predicts the next position of each tracked target; Feature Extractor \u2013 Siamese CNN, Motion Predictor \u2013 Kalman Filtering\nAffinity stage: feature vectors and motion predictions are used to compute a similarity/distance score between pairs of detections and/or tracklets; Mahalanobis/Cosine Distance\nAssociation stage: the similarity/distance measures are used to associate detections and tracklets belonging to the same target by assigning the same ID to detections that identify the same target; Hungarian Algorithm"),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"model flow",src:a(5586).Z,width:"1644",height:"844"})),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Figure 3. Model Flow")),(0,o.kt)("p",null,"As this post delivers high-level explanation about the model, if you want a more specified explanation or would like to play with the model please visit: ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/heejae1213/object_tracking_deepsort"},"https://github.com/heejae1213/object_tracking_deepsort")," (Have code but readmes are in progress)"),(0,o.kt)("p",null,"And below is the result of tracking on the video from Argonne Main Gate."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"traffic tracking video argonne ",src:a(5299).Z,width:"1440",height:"1080"})),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"Figure 4. Tracking on the video from Argonne Main Gate")),(0,o.kt)("h2",{id:"conclusion"},"Conclusion"),(0,o.kt)("p",null,"In conclusion, this summer, I could implement the Deep SORT-based vehicle tracker, and run it on the videos obtained from Argonne Main Gate camera.\nFor future work, I\u2019d like to make the model work in a near real-time manner and deploy it at the edge of Sage. Also, after successfully deploying the model at a single Sage node, expanding the model to multiple Sage nodes would allow tracking vehicles across certain space.\nThere are a lot of improvements that could be done on this topic, and I\u2019m looking forward to making a better version model!"),(0,o.kt)("p",null,"I am always open to comments, feedbacks, and questions! If you\u2019d like to reach out to me, feel free to connect me on Linkedin. \ud83d\ude42"),(0,o.kt)("h2",{id:"references"},"References"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Wojke, N., Bewley, A., & Paulus, D. (2017, September). Simple online and realtime tracking with a deep association metric. Proceedings from 2017 IEEE international conference on image processing (ICIP) (pp. 3645-3649). IEEE. ",(0,o.kt)("a",{parentName:"li",href:"https://doi.org/10.1109/ICIP.2017.8296962"},"https://doi.org/10.1109/ICIP.2017.8296962")),(0,o.kt)("li",{parentName:"ol"},"Beckman, P., Sankaran, R., Catlett, C., Ferrier, N., Jacob, R., & Papka, M. (2016, October). Waggle: An open sensor platform for edge computing. Proceedings from 2016 IEEE SENSORS (pp. 1-3). IEEE. ",(0,o.kt)("a",{parentName:"li",href:"https://doi.org/10.1109/ICSENS.2016.7808975"},"https://doi.org/10.1109/ICSENS.2016.7808975")),(0,o.kt)("li",{parentName:"ol"},"Ciaparrone, G., S\xe1nchez, F. L., Tabik, S., Troiano, L., Tagliaferri, R., & Herrera, F. (2020). Deep learning in video multi-object tracking: A survey. Neurocomputing, 381, 61-88. ",(0,o.kt)("a",{parentName:"li",href:"https://doi.org/10.1016/j.neucom.2019.11.023"},"https://doi.org/10.1016/j.neucom.2019.11.023")),(0,o.kt)("li",{parentName:"ol"},"Su, K., Li, J., & Fu, H. (2011, September). Smart city and the applications. Proceedings from 2011 international conference on electronics, communications and control (ICECC) (pp. 1028-1031). IEEE. ",(0,o.kt)("a",{parentName:"li",href:"https://doi.org/10.1109/ICECC.2011.6066743"},"https://doi.org/10.1109/ICECC.2011.6066743")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"https://github.com/nwojke/deep_sort"},"https://github.com/nwojke/deep_sort")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"https://github.com/abhyantrika/nanonets_object_tracking"},"https://github.com/abhyantrika/nanonets_object_tracking")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"https://github.com/nandinib1999/object-detection-yolo-opencv"},"https://github.com/nandinib1999/object-detection-yolo-opencv")),(0,o.kt)("li",{parentName:"ol"},(0,o.kt)("a",{parentName:"li",href:"https://youtu.be/UM0hX7nomi8"},"https://youtu.be/UM0hX7nomi8"))))}d.isMDXComponent=!0},7567:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/vehicle-tracking-1-d211b4d19f462f8a670bf7f436f7d3bf.gif"},7734:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/vehicle-tracking-2-cf3385beb53a3feb48842fe5ccae754a.png"},5586:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/vehicle-tracking-3-47fc689245be6220d35bbd3f886b30c0.png"},5299:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/vehicle-tracking-4-2c2bc902d3d76ded72a9398a71f39d70.gif"}}]);