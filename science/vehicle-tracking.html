<!doctype html>
<html lang="en" dir="ltr" class="mdx-wrapper mdx-page plugin-pages plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.3.1">
<title data-rh="true">Vehicle Tracking | Sage Website</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://nconrad.github.io/waggle-docs/science/vehicle-tracking"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Vehicle Tracking | Sage Website"><meta data-rh="true" name="description" content="Hi there!"><meta data-rh="true" property="og:description" content="Hi there!"><link data-rh="true" rel="icon" href="/waggle-docs/img/sage-favicon.png"><link data-rh="true" rel="canonical" href="https://nconrad.github.io/waggle-docs/science/vehicle-tracking"><link data-rh="true" rel="alternate" href="https://nconrad.github.io/waggle-docs/science/vehicle-tracking" hreflang="en"><link data-rh="true" rel="alternate" href="https://nconrad.github.io/waggle-docs/science/vehicle-tracking" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://MPDJQF6T11-dsn.algolia.net" crossorigin="anonymous"><link rel="alternate" type="application/rss+xml" href="/waggle-docs/blog/rss.xml" title="Sage Website RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/waggle-docs/blog/atom.xml" title="Sage Website Atom Feed">



<link rel="search" type="application/opensearchdescription+xml" title="Sage Website" href="/waggle-docs/opensearch.xml"><link rel="stylesheet" href="/waggle-docs/assets/css/styles.4db628e7.css">
<link rel="preload" href="/waggle-docs/assets/js/runtime~main.936cd2cc.js" as="script">
<link rel="preload" href="/waggle-docs/assets/js/main.89565177.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/waggle-docs/"><div class="navbar__logo"><img src="/waggle-docs/img/logo.png" alt="Site Logo" class="themedImage_ToTc themedImage--light_HNdA custom-navbar-logo-class"><img src="/waggle-docs/img/logo_dark.svg" alt="Site Logo" class="themedImage_ToTc themedImage--dark_i4oU custom-navbar-logo-class"></div><b class="navbar__title text--truncate">Sage</b></a><a class="navbar__item navbar__link" href="/waggle-docs/about">About</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/waggle-docs/science">Science</a><a class="navbar__item navbar__link" href="/waggle-docs/publications">Publications</a><a class="navbar__item navbar__link" href="/waggle-docs/blog">News</a><a class="navbar__item navbar__link" href="/waggle-docs/team">Team</a><a class="navbar__item navbar__link" href="/waggle-docs/docs/about/overview">Docs</a></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link header-github-link" aria-label="GitHub repository"></a><ul class="dropdown__menu"><li><a href="https://github.com/sagecontinuum" target="_blank" rel="noopener noreferrer" class="dropdown__link">Sage GitHub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li><a href="https://github.com/waggle-sensor" target="_blank" rel="noopener noreferrer" class="dropdown__link">Waggle Sensor GitHub<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="SignInBtn__Root-sc-1lj79ef-0 fqsrYB"><style data-emotion="css 6ii3fu">.css-6ii3fu{font-family:"Roboto","Helvetica","Arial",sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:5px 15px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border:1px solid rgba(25, 118, 210, 0.5);color:#1976d2;}.css-6ii3fu:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(25, 118, 210, 0.04);border:1px solid #1976d2;}@media (hover: none){.css-6ii3fu:hover{background-color:transparent;}}.css-6ii3fu.Mui-disabled{color:rgba(0, 0, 0, 0.26);border:1px solid rgba(0, 0, 0, 0.12);}</style><style data-emotion="css 79xub">.css-79xub{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;box-sizing:border-box;-webkit-tap-highlight-color:transparent;background-color:transparent;outline:0;border:0;margin:0;border-radius:0;padding:0;cursor:pointer;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;vertical-align:middle;-moz-appearance:none;-webkit-appearance:none;-webkit-text-decoration:none;text-decoration:none;color:inherit;font-family:"Roboto","Helvetica","Arial",sans-serif;font-weight:500;font-size:0.875rem;line-height:1.75;letter-spacing:0.02857em;text-transform:uppercase;min-width:64px;padding:5px 15px;border-radius:4px;-webkit-transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:background-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,box-shadow 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,border-color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms,color 250ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;border:1px solid rgba(25, 118, 210, 0.5);color:#1976d2;}.css-79xub::-moz-focus-inner{border-style:none;}.css-79xub.Mui-disabled{pointer-events:none;cursor:default;}@media print{.css-79xub{-webkit-print-color-adjust:exact;color-adjust:exact;}}.css-79xub:hover{-webkit-text-decoration:none;text-decoration:none;background-color:rgba(25, 118, 210, 0.04);border:1px solid #1976d2;}@media (hover: none){.css-79xub:hover{background-color:transparent;}}.css-79xub.Mui-disabled{color:rgba(0, 0, 0, 0.26);border:1px solid rgba(0, 0, 0, 0.12);}</style><a class="MuiButtonBase-root MuiButton-root MuiButton-outlined MuiButton-outlinedPrimary MuiButton-sizeMedium MuiButton-outlinedSizeMedium MuiButton-root MuiButton-outlined MuiButton-outlinedPrimary MuiButton-sizeMedium MuiButton-outlinedSizeMedium css-79xub" tabindex="0" href="https://portal.sagecontinuum.org">Portal<style data-emotion="css 1n4a93h">.css-1n4a93h{display:inherit;margin-right:-4px;margin-left:8px;}.css-1n4a93h>*:nth-of-type(1){font-size:20px;}</style><span class="MuiButton-endIcon MuiButton-iconSizeMedium css-1n4a93h"><style data-emotion="css vubbuv">.css-vubbuv{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:1em;height:1em;display:inline-block;fill:currentColor;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;-webkit-transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;transition:fill 200ms cubic-bezier(0.4, 0, 0.2, 1) 0ms;font-size:1.5rem;}</style><svg class="MuiSvgIcon-root MuiSvgIcon-fontSizeMedium css-vubbuv" focusable="false" aria-hidden="true" viewBox="0 0 24 24" data-testid="LoginRoundedIcon"><path d="M10.3 7.7c-.39.39-.39 1.01 0 1.4l1.9 1.9H3c-.55 0-1 .45-1 1s.45 1 1 1h9.2l-1.9 1.9c-.39.39-.39 1.01 0 1.4.39.39 1.01.39 1.4 0l3.59-3.59c.39-.39.39-1.02 0-1.41L11.7 7.7a.9839.9839 0 0 0-1.4 0zM20 19h-7c-.55 0-1 .45-1 1s.45 1 1 1h7c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2h-7c-.55 0-1 .45-1 1s.45 1 1 1h7v14z"></path></svg></span></a></div><div class="searchBox_ZlJk"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><main class="container container--fluid margin-vert--lg"><div class="row mdxPageWrapper_j9I6"><div class="col col--8"><article><h1>Vehicle Tracking</h1><p>Hi there!
My name is Hazel Han, and I‚Äôm a graduate student at Purdue University pursuing MS in Computer and Information Technology.
This summer, I worked as a research intern at Argonne National Laboratory on a ‚ÄúVehicle Tracking‚Äù project, pursuing the ultimate goal of running tracker at the edge of the smart sensor network, and I‚Äôd like to share what I‚Äôve done throughout the summer.</p><p>#Smart City? Urban Surveillance?
Vehicle is one of the most useful and indispensable machines in our lives. Every day people drive to work, goods are transported by vehicles, and people even play sports with vehicles. Seriously, I can‚Äôt even imagine life without vehicles. However, as they are so close to human lives, there are also multifarious problems that vehicles are involved with, such as traffic congestion, vehicle theft and trafficking, and hit-and-run. One way to deal with those vehicle-related problems is to track each vehicle. An intelligent traffic surveillance system that could track each vehicle and figure out the trajectory of each vehicle will play a significant role in various tasks such as traffic flow analysis or catching hit-and-run criminals. Therefore, based on such needs for an intelligent traffic surveillance system, I‚Äôve conducted research on developing a near real-time vehicle tracking model that could be used towards the intelligent traffic surveillance system.</p><p>With the growth of the Internet of Things (IoT) related technology, a lot of governments, national laboratories, and companies are to construct a smart city infrastructure these days. (Su, Li &amp; Fu, 2011). Argonne National Laboratory is one of those institutions. Beckman et al. at Argonne National Laboratory have been developing an open platform named Sage, a smart software-defined sensor network, that could be used for various civil applications and AI-related researches. So during this summer, I worked on implementing a deep learning-based vehicle tracking model that could be deployed at the edge of Sage, and I‚Äôd like to share a high-level description about my model in this post.</p><p><img loading="lazy" alt="tracking traffic" src="/waggle-docs/assets/images/vehicle-tracking-1-d211b4d19f462f8a670bf7f436f7d3bf.gif" width="1280" height="720" class="img_ev3q"></p><blockquote><p>Figure 1. Tracking on random traffic video from YouTube</p></blockquote><h1>Multiple Object Tracking</h1><p>Before we go into details of the model I‚Äôve implemented, I‚Äôd like to first introduce how the general computer vision-based tracking algorithms work. Although there are various approaches for developing computer vision-based tracking systems, the gist of tracking algorithms is pretty much the same for every tracking algorithms.
Most of the tracking systems that have been proposed since 1990s are feature-based tracking systems, which use feature vectors extracted from the image frames to associate certain feature vector from certain image frame with another similar feature vector extracted from another frame.
The general tracking scenario is as follows:</p><p>Feature extraction from each frame
Measure the similarity between feature vectors
Associate those similar vectors with unique tracking ID (similar feature vector = the same object)
Having this basic scenario, various methods and techniques have been explored by researchers to come up with better performing tracking algorithms. However, while the object detection task has been continuously addressed by the computer vision community with well-organized benchmarks for the task since the early 2010s (ImageNet), there has been rather limited work on the task of object tracking although it can be used for various applications. To address such issue, Leal-Taixe ÃÅ et al. have launched Multiple Object Tracking Challenge (MOTChallenge) in 2015, in order to provide a novel multiple object tracking benchmark, which consists of a large-scale well-annotated dataset and a unified evaluation framework for effective quantification of multi-object tracking.</p><p>Since the first MOTChallenge, a significant number of researchers have participated and contributed to the challenge, proposing state-of-the-art tracking algorithms, and there is an obvious trend in submitted algorithms. Most of the MOT algorithms take tracking-by-detection approach.
To be more specific, a set of detections (i.e. bounding boxes identifying the targets in the image) are extracted from the video frames and are used to guide the tracking process, usually by associating them together in order to assign the same ID to bounding boxes that contain the same target.
Let‚Äôs take an example of a pedestrian tracking shown in the below figure:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tracking-by-detection">Tracking-by-Detection<a href="#tracking-by-detection" class="hash-link" aria-label="Direct link to Tracking-by-Detection" title="Direct link to Tracking-by-Detection">‚Äã</a></h3><p><img loading="lazy" alt="workflow of MOT algorithms" src="/waggle-docs/assets/images/vehicle-tracking-2-cf3385beb53a3feb48842fe5ccae754a.png" width="1226" height="652" class="img_ev3q"></p><blockquote><p>Figure 2. Usual workflow of MOT Algorithms (Ciaparrone et al., 2020)</p></blockquote><p>Given the raw frames of a video (1), an object detector is run to obtain the bounding boxes of the objects (2). Then, for every detected object, different features are computed, usually visual and motion ones (3). After that, an affinity computation step calculates the probability of two objects belonging to the same target (4), and finally an association step assigns a numerical ID to each object (5). (Ciapparrone et al., 2020)</p><p>So this is how most of the current state-of-the-art tracking algorithms work.</p><h1>Deep SORT based Vehicle Tracker</h1><p>Now I‚Äôd like to introduce a specific tracker, Deep SORT, that I used as a baseline. Deep SORT is one of the fastest trackers that takes a pragmatic approach to multiple object tracking with a focus on simple, effective algorithms. (Wojke et al., 2017). Following the general tracking scenario that I just described in the previous section, Deep SORT utilizes Kalman Filtering and Hungarian Algorithm to expedite the speed of tracker. Based on Deep SORT, which intends to track pedestrians from the given videos, I have customized and implemented the tracking model which tracks vehicles.
The detailed model flow is depicted in Figure 3. The model has four stages ‚Äì detection stage, feature extraction/motion prediction stage, affinity stage, and association stage ‚Äì and here is the detailed explanation about each stage:</p><p>Detection stage: detect objects from each input frame with state-of-the-art object detectors, and generate bounding boxes around the detected objects; YOLOv3
Feature extraction/motion prediction stage: a feature extractor extracts feature vector of target objects (NVIDIA AI City Challenge Re-Identification dataset used), and a motion predictor predicts the next position of each tracked target; Feature Extractor ‚Äì Siamese CNN, Motion Predictor ‚Äì Kalman Filtering
Affinity stage: feature vectors and motion predictions are used to compute a similarity/distance score between pairs of detections and/or tracklets; Mahalanobis/Cosine Distance
Association stage: the similarity/distance measures are used to associate detections and tracklets belonging to the same target by assigning the same ID to detections that identify the same target; Hungarian Algorithm</p><p><img loading="lazy" alt="model flow" src="/waggle-docs/assets/images/vehicle-tracking-3-47fc689245be6220d35bbd3f886b30c0.png" width="1644" height="844" class="img_ev3q"></p><blockquote><p>Figure 3. Model Flow</p></blockquote><p>As this post delivers high-level explanation about the model, if you want a more specified explanation or would like to play with the model please visit: <a href="https://github.com/heejae1213/object_tracking_deepsort" target="_blank" rel="noopener noreferrer">https://github.com/heejae1213/object_tracking_deepsort</a> (Have code but readmes are in progress)</p><p>And below is the result of tracking on the video from Argonne Main Gate.</p><p><img loading="lazy" alt="traffic tracking video argonne " src="/waggle-docs/assets/images/vehicle-tracking-4-2c2bc902d3d76ded72a9398a71f39d70.gif" width="1440" height="1080" class="img_ev3q"></p><blockquote><p>Figure 4. Tracking on the video from Argonne Main Gate</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">‚Äã</a></h2><p>In conclusion, this summer, I could implement the Deep SORT-based vehicle tracker, and run it on the videos obtained from Argonne Main Gate camera.
For future work, I‚Äôd like to make the model work in a near real-time manner and deploy it at the edge of Sage. Also, after successfully deploying the model at a single Sage node, expanding the model to multiple Sage nodes would allow tracking vehicles across certain space.
There are a lot of improvements that could be done on this topic, and I‚Äôm looking forward to making a better version model!</p><p>I am always open to comments, feedbacks, and questions! If you‚Äôd like to reach out to me, feel free to connect me on Linkedin. üôÇ</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">‚Äã</a></h2><ol><li>Wojke, N., Bewley, A., &amp; Paulus, D. (2017, September). Simple online and realtime tracking with a deep association metric. Proceedings from 2017 IEEE international conference on image processing (ICIP) (pp. 3645-3649). IEEE. <a href="https://doi.org/10.1109/ICIP.2017.8296962" target="_blank" rel="noopener noreferrer">https://doi.org/10.1109/ICIP.2017.8296962</a></li><li>Beckman, P., Sankaran, R., Catlett, C., Ferrier, N., Jacob, R., &amp; Papka, M. (2016, October). Waggle: An open sensor platform for edge computing. Proceedings from 2016 IEEE SENSORS (pp. 1-3). IEEE. <a href="https://doi.org/10.1109/ICSENS.2016.7808975" target="_blank" rel="noopener noreferrer">https://doi.org/10.1109/ICSENS.2016.7808975</a></li><li>Ciaparrone, G., S√°nchez, F. L., Tabik, S., Troiano, L., Tagliaferri, R., &amp; Herrera, F. (2020). Deep learning in video multi-object tracking: A survey. Neurocomputing, 381, 61-88. <a href="https://doi.org/10.1016/j.neucom.2019.11.023" target="_blank" rel="noopener noreferrer">https://doi.org/10.1016/j.neucom.2019.11.023</a></li><li>Su, K., Li, J., &amp; Fu, H. (2011, September). Smart city and the applications. Proceedings from 2011 international conference on electronics, communications and control (ICECC) (pp. 1028-1031). IEEE. <a href="https://doi.org/10.1109/ICECC.2011.6066743" target="_blank" rel="noopener noreferrer">https://doi.org/10.1109/ICECC.2011.6066743</a></li><li><a href="https://github.com/nwojke/deep_sort" target="_blank" rel="noopener noreferrer">https://github.com/nwojke/deep_sort</a></li><li><a href="https://github.com/abhyantrika/nanonets_object_tracking" target="_blank" rel="noopener noreferrer">https://github.com/abhyantrika/nanonets_object_tracking</a></li><li><a href="https://github.com/nandinib1999/object-detection-yolo-opencv" target="_blank" rel="noopener noreferrer">https://github.com/nandinib1999/object-detection-yolo-opencv</a></li><li><a href="https://youtu.be/UM0hX7nomi8" target="_blank" rel="noopener noreferrer">https://youtu.be/UM0hX7nomi8</a></li></ol></article></div><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#tracking-by-detection" class="table-of-contents__link toc-highlight">Tracking-by-Detection</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></main></div><footer class="footer"><div class="container container-fluid"></div></footer></div>
<script src="/waggle-docs/assets/js/runtime~main.936cd2cc.js"></script>
<script src="/waggle-docs/assets/js/main.89565177.js"></script>
</body>
</html>